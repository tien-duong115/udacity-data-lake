# README Project Overview

- The project goal is to build ETL PIPEline for an make-up music streaming website name `Sparkify`. The start up accumualted large amount of data, and now they want to move it from data warehouse to a data lake. Sparkify data current resides at S3, we will extract out these data by building an ETL pipeline and load back into S3 as a set of dimensional tables.

## Database Schema Design
- Contain 1 fact table name `songplays`
- Contain 4 dimension tables name `users`, `song`, `artists`, `time`
These Table will load in model of Star Schema  which contain cleaned data that'll assist analytic team in finding insight

## ETL PipeLine Description
The pipeline will setup to perform extraction data from S3, 
With spark, we'll use to process the data and tranform it into suitable format before load back into S3

## DataSet info
Song data: s3://udacity-dend/song_data
 - Each file is in JSON format and contains metadata about a song and the artist of that song.
 
Log data: s3://udacity-dend/log_data
-  log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.


### Current Project Files
 
 1. ELT.py  --> Responsible for execution of extract, tranform and load the dataset from S3 back into S3 processes using Spark
 
 2. dl.cfg --> Contains AWS credentials
 